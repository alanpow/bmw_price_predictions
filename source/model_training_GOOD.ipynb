{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6fa711e-0661-42bf-a1e3-f442a05e0edc",
   "metadata": {},
   "source": [
    "# All steps for fitting best model (Final Working .pkl files)\n",
    "\n",
    "After doing some modeling and feature engineering post EDA, I will now implement the best performing model in this notebook as well as prepare the joblib pipelining for the feature engineering. We will have all the scalers/encoders/transformers for the data serialized as well as the model, so that we can make use of it with the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8e680f-7949-4cdd-9f86-d31c90d5fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Find the current working directory\n",
    "current_dir = Path().resolve()\n",
    "\n",
    "# Traverse upwards to find the root of the repository\n",
    "repo_root = current_dir\n",
    "while not (repo_root / '.git').exists():\n",
    "    if repo_root == repo_root.parent:\n",
    "        raise FileNotFoundError(\"Repository root with .git directory not found\")\n",
    "    repo_root = repo_root.parent\n",
    "\n",
    "# Set the working directory to the root of the repository\n",
    "os.chdir(repo_root)\n",
    "\n",
    "# Load the dataset\n",
    "data_path = repo_root / 'data/bmw_pricing_challenge.csv'\n",
    "source_dir = repo_root / 'source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92abe74-19fd-4c18-8750-e0d674d89e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alala\\miniconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['source\\\\model.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "training_data = pd.read_csv(data_path)\n",
    "\n",
    "# Convert 'registration_date' and 'sold_at' to datetime\n",
    "training_data['registration_date'] = pd.to_datetime(training_data['registration_date'])\n",
    "training_data['sold_at'] = pd.to_datetime(training_data['sold_at'])\n",
    "\n",
    "# Extract year and month from 'registration_date' and 'sold_at'\n",
    "training_data['registration_year'] = training_data['registration_date'].dt.year\n",
    "training_data['registration_month'] = training_data['registration_date'].dt.month\n",
    "training_data['sold_year'] = training_data['sold_at'].dt.year\n",
    "training_data['sold_month'] = training_data['sold_at'].dt.month\n",
    "\n",
    "# Function to categorize models\n",
    "def categorize_model(model):\n",
    "    entry_level = ['114', '116', '118', '120', '123', '125', '135', '216', '218', '220', '225', 'X1', 'X2', 'i3', 'Z4']\n",
    "    middle_level = ['316', '318', '320', '325', '328', '330', '335', '418', '420', '425', '430', '435', '518', '520', '523', '525', '528', '530', '535', 'X3', 'X4', 'i4', 'i5']\n",
    "    high_end = ['630', '635', '640', '650', '730', '735', '740', '750', '8', 'X5', 'X6', 'X7', 'M135', 'M235', 'M3', 'M4', 'M5', 'M550', 'i7', 'i8']\n",
    "    \n",
    "    if any(model.startswith(prefix) for prefix in entry_level):\n",
    "        return 'entry level'\n",
    "    elif any(model.startswith(prefix) for prefix in middle_level):\n",
    "        return 'middle level'\n",
    "    elif any(model.startswith(prefix) for prefix in high_end):\n",
    "        return 'high end'\n",
    "    else:\n",
    "        return 'middle level'  # Default to middle level if not found\n",
    "\n",
    "# Apply the model categorization\n",
    "training_data['model_category'] = training_data['model_key'].apply(categorize_model)\n",
    "\n",
    "# Define the features\n",
    "categorical_features = ['maker_key', 'model_key', 'fuel', 'paint_color', 'car_type', 'model_category']\n",
    "numerical_features = ['mileage', 'engine_power', 'registration_year', 'registration_month', 'sold_year', 'sold_month']\n",
    "\n",
    "# OneHotEncoder for categorical features\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "ohe.fit(training_data[categorical_features])\n",
    "\n",
    "# StandardScaler for numerical features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(training_data[numerical_features])\n",
    "\n",
    "# PolynomialFeatures for interaction terms\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly.fit(scaler.transform(training_data[numerical_features]))\n",
    "\n",
    "# Save the encoders and scaler for later use\n",
    "source_dir = 'source'\n",
    "joblib.dump(ohe, Path(source_dir) / 'ohe.pkl')\n",
    "joblib.dump(scaler, Path(source_dir) / 'scaler.pkl')\n",
    "joblib.dump(poly, Path(source_dir) / 'poly.pkl')\n",
    "\n",
    "# Transform the training data\n",
    "X_categorical = ohe.transform(training_data[categorical_features])\n",
    "X_numerical = scaler.transform(training_data[numerical_features])\n",
    "X_poly = poly.transform(X_numerical)\n",
    "X = np.hstack((X_categorical, X_poly))\n",
    "y = training_data['price']  # Replace 'price' with the actual target column\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, Path(source_dir) / 'model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e97150e2-1151-45cf-a96e-7f87c6a92f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23716.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the saved transformers and model\n",
    "source_dir = Path('source')\n",
    "ohe = joblib.load(source_dir / 'ohe.pkl')\n",
    "scaler = joblib.load(source_dir / 'scaler.pkl')\n",
    "poly = joblib.load(source_dir / 'poly.pkl')\n",
    "model = joblib.load(source_dir / 'model.pkl')\n",
    "\n",
    "# Example input data for prediction\n",
    "input_data = pd.DataFrame([{\n",
    "    'maker_key': 'BMW',\n",
    "    'model_key': '320i',\n",
    "    'mileage': 20000,\n",
    "    'engine_power': 150,\n",
    "    'registration_date': '2017-05-05',\n",
    "    'fuel': 'petrol',\n",
    "    'paint_color': 'black',\n",
    "    'car_type': 'sedan',\n",
    "    'sold_at': '2018-05-05'\n",
    "}])\n",
    "\n",
    "# Convert 'registration_date' and 'sold_at' to datetime\n",
    "input_data['registration_date'] = pd.to_datetime(input_data['registration_date'])\n",
    "input_data['sold_at'] = pd.to_datetime(input_data['sold_at'])\n",
    "\n",
    "# Extract year and month from 'registration_date' and 'sold_at'\n",
    "input_data['registration_year'] = input_data['registration_date'].dt.year\n",
    "input_data['registration_month'] = input_data['registration_date'].dt.month\n",
    "input_data['sold_year'] = input_data['sold_at'].dt.year\n",
    "input_data['sold_month'] = input_data['sold_at'].dt.month\n",
    "\n",
    "# Apply the model categorization\n",
    "input_data['model_category'] = input_data['model_key'].apply(categorize_model)\n",
    "\n",
    "# Define the features\n",
    "categorical_features = ['maker_key', 'model_key', 'fuel', 'paint_color', 'car_type', 'model_category']\n",
    "numerical_features = ['mileage', 'engine_power', 'registration_year', 'registration_month', 'sold_year', 'sold_month']\n",
    "\n",
    "# Transform the input data\n",
    "X_categorical = ohe.transform(input_data[categorical_features])\n",
    "X_numerical = scaler.transform(input_data[numerical_features])\n",
    "X_poly = poly.transform(X_numerical)\n",
    "\n",
    "# Concatenate categorical and numerical features\n",
    "X = np.hstack((X_categorical, X_poly))\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
